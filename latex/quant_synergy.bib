%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Know at 2011-06-06 17:46:48 -0700


%% Saved with string encoding Western (Mac OS Roman)

@article{Iwedge,
   author = {{Griffith}, V. and {Chong}, E.~K.~P. and {James}, R.~G. and 
	{Ellison}, C.~J. and {Crutchfield}, J.~P.},
    title = {Intersection Information based on Common Randomness},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1310.1538},
 primaryClass = "cs.IT",
 keywords = {Computer Science - Information Theory},
     year = 2013,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1310.1538G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{adami97,
  title = {Negative Entropy and Information in Quantum Mechanics},
  author = {Cerf, N. J. and Adami, C.},
  journal = {Phys. Rev. Lett.},
  volume = {79},
  issue = {26},
  pages = {5194--5197},
  year = {1997},
  month = {Dec},
  doi = {10.1103/PhysRevLett.79.5194},
  url = {http://link.aps.org/doi/10.1103/PhysRevLett.79.5194},
  publisher = {American Physical Society}
}

@article{lizier13,
  author    = {Joseph T. Lizier and
               Benjamin Flecker and
               Paul L. Williams},
  title     = {Towards a Synergy-based Approach to Measuring Information
               Modification},
  journal   = {CoRR},
  volume    = {abs/1303.3440},
  year      = {2013},
  ee        = {http://arxiv.org/abs/1303.3440},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@INPROCEEDINGS{liu10,
author={Wei Liu and Ge Xu and Biao Chen},
booktitle={Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on},
title={The common information of N dependent random variables},
year={2010},
pages={836-843},
doi={10.1109/ALLERTON.2010.5706995}
}

@article{klir04,
    title = {Generalized information theory: aims, results, and open problems},
    journal = {Reliability Engineering and System Safety},
    volume = {85},
    number = {1–3},
    pages = {21--38},
    year = {2004},
    note = {Alternative Representations of Epistemic Uncertainty},
    issn = {0951-8320},
    doi = {http://dx.doi.org/10.1016/j.ress.2004.03.003},
    url = {http://www.sciencedirect.com/science/article/pii/S095183200400050X},
    author = {George J. Klir}
}


@article{li11,
    AUTHOR = {Li, Hua and Chong, Edwin K. P.},
    TITLE = {On a Connection between Information and Group Lattices},
    JOURNAL = {Entropy},
    VOLUME = {13},
    YEAR = {2011},
    NUMBER = {3},
    PAGES = {683--708},
    URL = {http://www.mdpi.com/1099-4300/13/3/683},
    ISSN = {1099-4300},
    DOI = {10.3390/e13030683}
}



@inproceedings{qsmi,
    Author={Virgil Griffith and Christof Koch},
    booktitle={Guided Self-Organization: Inception},
    title={Quantifying Synergistic Mutual Information},
    year={2014},
    publisher = {Springer},
    editor = {M. Prokopenko}
    }



@article{bertschinger12,
  author    = {Nils Bertschinger and
               Johannes Rauh and
               Eckehard Olbrich and
               J{\"u}rgen Jost},
  title     = {Shared Information -- New Insights and Problems in Decomposing
               Information in Complex Systems},
  journal   = {CoRR},
  volume    = {abs/1210.5902},
  year      = {2012},
  ee        = {http://arxiv.org/abs/1210.5902},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{amari99,
    author = {Shun-ichi Amari},
    title = {Information Geometry on Hierarchical Decomposition of Stochastic Interactions},
    journal = {IEEE Transaction on Information Theory},
    year = {1999},
    volume = {47},
    pages = {1701--1711}
}

@article{synredunn2,
  author    = {Virgil Griffith},
  title     = {Bivariate redundancy and synergy, or: understanding conditional mutual information},
  journal   = {in press},
  volume    = {},
  year      = {2012},
  ee        = {},
  bibsource = {}
}


@article{polani12,
  author    = {Malte Harder and Christoph Salge and Daniel Polani},
  title     = {A Bivariate Measure of Redundant Information},
  journal   = {CoRR},
  volume    = {abs/1207.2080},
  year      = {2012},
  ee        = {http://arxiv.org/abs/1207.2080},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{white11,
    Author={White, D. and Rabago-Smith, M.},
    Title={Genotype-phenotype associations and human eye color},
    year={2011},
    Month={January},
    Number={1},
    Volume={56},
    Journal={Journal of Human Genetics},
    Pages={5--7}
}

@inproceedings{wolf03,
    Author={Christandl, M. and Renner, R. and Wolf, S.},
    booktitle={Proceedings of the IEEE International Symposium on Information Theory},
    title={A property of the intrinsic mutual information},
    year={2003},
    month={June},
    pages={258},
    doi={10.1109/ISIT.2003.1228272}
    }

@article{maurer99,
    Author={U. M. Maurer and S. Wolf},
    Title={Unconditionally Secure Key Agreement and the Intrinsic Conditional Information},
    Journal={IEEE Transactions on Information Theory},
    Year={1999},
    Volume={45},
    Pages={499-514},
    Number={2}
    }

@article{frontiers2011,
    Author={Marshall Crumiller and  Bruce Knight and Yunguo Yu and Ehud Kaplan},
    Journal={Frontiers in Neuroscience},
    Title={Estimating the amount of information conveyed by a population of neurons},
    Volume={5},
    Year={2011},
    Number={00090},
    DOI={10.3389/fnins.2011.00090},
    ISSN={1662-453X},
    URL={http://www.frontiersin.org/Journal/Abstract.aspx?f=55&name=neuroscience&ART_DOI=10.3389/fnins.2011.00090}
    }

@article{narayanan05,
    Author = {Narayanan, Nandakumar S. and Kimchi, Eyal Y. and Laubach, Mark},
    Date-Added = {2011-06-06 15:34:01 -0700},
    Date-Modified = {2011-06-06 15:34:01 -0700},
    Doi = {10.1523/JNEUROSCI.4697-04.2005},
    Eprint = {http://www.jneurosci.org/content/25/17/4207.full.pdf+html},
    Journal = {The Journal of Neuroscience},
    Number = {17},
    Pages = {4207-4216},
    Title = {Redundancy and Synergy of Neuronal Ensembles in Motor Cortex},
    Url = {http://www.jneurosci.org/content/25/17/4207.abstract},
    Volume = {25},
    Year = {2005},
    Bdsk-Url-1 = {http://www.jneurosci.org/content/25/17/4207.abstract},
    Bdsk-Url-2 = {http://dx.doi.org/10.1523/JNEUROSCI.4697-04.2005}}


@incollection {korner06,
   author = {R. Ahlswede and J. Körner},
   affiliation = {Fakultät für Mathematik, Universität Bielefeld, Universitätsstr. 25, 33615 Bielefeld, Germany},
   title = {Appendix: On Common Information and Related Characteristics of Correlated Information Sources},
   booktitle = {General Theory of Information Transfer and Combinatorics},
   series = {Lecture Notes in Computer Science},
   editor = {Ahlswede, Rudolf and Bäumer, Lars and Cai, Ning and Aydinian, Harout and Blinovsky, Vladimir and Deppe, Christian and Mashurian, Haik},
   publisher = {Springer Berlin},
   isbn = {978-3-540-46244-6},
   pages = {664--677},
   volume = {4123},
   year = {2006}
}

@article{wyner75,
    title ={The common information of two dependent random variables},
    volume={21},
    number={2},
    journal={IEEE Transactions in Information Theory},
    pages={163--179},
    year={1975},
    author={A. D. Wyner},
    month={March} }

@article{gacs73,
  title={Common information is far less than mutual information},
  volume={2},
  url={http://www.ams.org/mathscinet/search/publications.html?pg1=MR&s1=MR0356946},
  number={2},
  journal={Problems of Control and Informaton Theory},
  author={Peter Gács and Jack Körner},
  year={1973},
  pages={149--162}}

@article{wolf04,
  author    = {Stefan Wolf and Jürg Wullschleger},
  title     = {Zero-error information and applications in cryptography},
  journal   = {Proc IEEE Information Theory Workshop},
  volume    = {04},
  year      = {2004},
  ee        = {http://jurg.wulli.com/crypto/WolWul04.pdf},
  Pages   = {1--6} }

@article{tandon11,
  author    = {Ravi Tandon and Lalitha Sankar and H. Vincent Poor},
  title     = {Multi-User Privacy: The Gray-Wyner System and Generalized Common Information},
  journal   = {CoRR},
  volume    = {abs/1106.2050},
  year      = {2011},
  ee        = {http://arxiv.org/abs/1106.2050},
  bibsource = {DBLP, http://dblp.uni-trier.de} }

@article{richmond-93,
    Author={Timothy J. Gawne and Barry J. Richmond},
    Url={http://www.ncbi.nlm.nih.gov/pubmed/8331371},
    Year={1993},
    Journal={Journal of Neuroscience},
    Volume={13},
    Issue={7},
    Pages={2758-71},
    Title={How independent are the messages carried by adjacent inferior temporal cortical neurons?},
    Ee={http://www.ncbi.nlm.nih.gov/pubmed/8331371}}

@article{kamath-10,
    Author={S. Kamath and V. Anantharam},
    Url={http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5707069&tag=1},
    Year={2010},
    Pages={1340-46},
    ISBN={978-1-4244-8215-3},
    Journal={Forty-Eighth Annual Allerton Conference on Communication, Control, and Computing},
    Title={A new dual to the Gács-Körner common information defined via the Gray-Wyner system},
    Ee={http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5707069&tag=1},
    doi={10.1109/ALLERTON.2010.5707069}}

@article{lei-10,
    Author = {Wei Lei and Ge Xu and Biao Chen},
    Url = {http://arxiv.org/abs/1010.3613},
    Year = {2010},
    Month= {9},
    Journal={Forty-Eighth Annual Allerton Conference on Communication, Control, and Computing},
    ISBN={978-1-4244-8215-3},
    Title={The Common Information of N Dependent Random Variables},
    Ee={http://arxiv.org/abs/1010.3613},
    Volume={abs/1010.3613},
    Pages={836--843}
    }

@article{plw-11,
    Author = {Paul L. Williams and Randall D. Beer},
    Url = {http://arxiv.org/abs/1102.1507v1},
    Year = {2011},
    Title = {Generalized Measures of Information Transfer},
    Journal = {arXiv:1102.1507v1},
    Ee = {http://arxiv.org/abs/1102.1507v1},
    Volume = {abs/1102.1507v1}
    }

@article{narayanan05,
    Author = {Narayanan, Nandakumar S. and Kimchi, Eyal Y. and Laubach, Mark},
    Date-Added = {2011-06-06 15:34:01 -0700},
    Date-Modified = {2011-06-06 15:34:01 -0700},
    Doi = {10.1523/JNEUROSCI.4697-04.2005},
    Eprint = {http://www.jneurosci.org/content/25/17/4207.full.pdf+html},
    Journal = {The Journal of Neuroscience},
    Number = {17},
    Pages = {4207-4216},
    Title = {Redundancy and Synergy of Neuronal Ensembles in Motor Cortex},
    Url = {http://www.jneurosci.org/content/25/17/4207.abstract},
    Volume = {25},
    Year = {2005},
    Bdsk-Url-1 = {http://www.jneurosci.org/content/25/17/4207.abstract},
    Bdsk-Url-2 = {http://dx.doi.org/10.1523/JNEUROSCI.4697-04.2005}}

@article{pola03,
    Abstract = {We derive a new method to quantify the impact of correlated firing on the information transmitted by neuronal populations. This new method considers, in an exact way, the effects of high order spike train statistics, with no approximation involved, and it generalizes our previous work that was valid for short time windows and small populations. The new technique permits one to quantify the information transmitted if each cell were to convey fully independent information separately from the information available in the presence of synergy-redundancy effects. Synergy-redundancy effects are shown to arise from three possible contributions: a redundant contribution due to similarities in the mean response profiles of different cells; a synergistic stimulus-dependent correlational contribution quantifying the information content of changes of correlations with stimulus, and a stimulus-independent correlational contribution term that reflects interactions between the distribution of rates of individual cells and the average level of cross-correlation. We apply the new method to simultaneously recorded data from somatosensory and visual cortices. We demonstrate that it constitutes a reliable tool to determine the role of cross-correlated activity in stimulus coding even when high firing rate data (such as multi-unit recordings) are considered.},
    Address = {Department of Psychology, University of Newcastle upon Tyne, The Henry Wellcome Building for Neuroecology, Medical School, Framlington Place, Newcastle upon Tyne NE2 4HH, UK.},
    Author = {Pola, G and Thiele, A and Hoffmann, K P and Panzeri, S},
    Crdt = {2003/03/05 04:00},
    Da = {20030304},
    Date = {2003 Feb},
    Date-Added = {2011-06-02 13:36:48 -0700},
    Date-Modified = {2011-06-02 13:36:48 -0700},
    Dcom = {20030320},
    Edat = {2003/03/05 04:00},
    Issn = {0954-898X (Print); 0954-898X (Linking)},
    Jid = {9431867},
    Journal = {Network},
    Jt = {Network (Bristol, England)},
    Language = {eng},
    Lr = {20071115},
    Mh = {Action Potentials; Computer Simulation; Information Theory; *Models, Neurological; Neural Conduction; Neural Networks (Computer); Neurons/*physiology; Reaction Time; Reproducibility of Results; Somatosensory Cortex/physiology; Statistics as Topic/methods; Time Factors},
    Mhda = {2003/03/21 04:00},
    Month = {Feb},
    Number = {1},
    Own = {NLM},
    Pages = {35--60},
    Pl = {England},
    Pmid = {12613551},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, Non-U.S. Gov't},
    Sb = {IM},
    Title = {An exact method to quantify the information transmitted by different mechanisms of correlational coding},
    Volume = {14},
    Year = {2003}}

@article{nirenberg03,
    Abstract = {It has been known for >30 years that neuronal spike trains exhibit  correlations, that is, the occurrence of a spike at one time is not  independent of the occurrence of spikes at other times, both within spike  trains from single neurons and across spike trains from multiple neurons. The  presence of these correlations has led to the proposal that they might form a  key element of the neural code. Specifically, they might act as an extra  channel for information, carrying messages about events in the outside world  that are not carried by other aspects of the spike trains, such as firing  rate. Currently, there is no general consensus about whether this proposal  applies to real spike trains in the nervous system. This is largely because it  has been hard to separate information carried in correlations from that not  carried in correlations. Here we propose a framework for performing this  separation. Specifically, we derive an information-theoretic cost function  that measures how much harder it is to decode neuronal responses when  correlations are ignored than when they are taken into account. This cost  function can be readily applied to real neuronal data.},
    Author = {Sheila Nirenberg and Peter E. Latham},
    Date-Added = {2011-06-02 13:36:48 -0700},
    Date-Modified = {2011-06-02 13:36:48 -0700},
    Doi = {10.1073/pnas.1131895100},
    Eprint = {http://www.pnas.org/content/100/12/7348.full.pdf+html},
    Journal = {Proceedings of the National Academy of Sciences},
    Number = {12},
    Pages = {7348--7353},
    Title = {Decoding neuronal spike trains: How important are correlations?},
    Url = {http://www.pnas.org/content/100/12/7348.abstract},
    Volume = {100},
    Year = {2003},
    Bdsk-Url-1 = {http://www.pnas.org/content/100/12/7348.abstract},
    Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.1131895100}}

@article{panzeri99,
    Abstract = {The effectiveness of various stimulus identification (decoding) procedures for extracting the information carried by the responses of a population of neurons to a set of repeatedly presented stimuli is studied analytically, in the limit of short time windows. It is shown that in this limit, the entire information content of the responses can sometimes be decoded, and when this is not the case, the lost information is quantified. In particular, the mutual information extracted by taking into account only the most likely stimulus in each trial turns out to be, if not equal, much closer to the true value than that calculated from all the probabilities that each of the possible stimuli in the set was the actual one. The relation between the mutual information extracted by decoding and the percentage of correct stimulus decodings is also derived analytically in the same limit, showing that the metric content index can be estimated reliably from a few cells recorded from brief periods. Computer simulations as well as the activity of real neurons recorded in the primate hippocampus serve to confirm these results and illustrate the utility and limitations of the approach.},
    Address = {Neural Systems Group, Department of Psychology, Ridley Building, University of Newcastle upon Tyne, Newcastle upon Tyne NE1 7RU, UK. stefano.panzeri@ncl.ac.uk},
    Author = {Panzeri, S and Treves, A and Schultz, S and Rolls, E T},
    Crdt = {1999/09/22 00:00},
    Da = {19991105},
    Date = {1999 Oct 1},
    Date-Added = {2011-06-02 13:36:48 -0700},
    Date-Modified = {2011-06-02 13:36:48 -0700},
    Dcom = {19991105},
    Edat = {1999/09/22},
    Issn = {0899-7667 (Print); 0899-7667 (Linking)},
    Jid = {9426182},
    Journal = {Neural Comput},
    Jt = {Neural computation},
    Language = {eng},
    Lr = {20061115},
    Mh = {Algorithms; Animals; Computer Simulation; Haplorhini; Models, Neurological; Neurons/*physiology; Parahippocampal Gyrus/cytology/physiology; Pyramidal Cells},
    Mhda = {1999/09/22 00:01},
    Month = {Oct},
    Number = {7},
    Own = {NLM},
    Pages = {1553--1577},
    Pl = {UNITED STATES},
    Pmid = {10490938},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, Non-U.S. Gov't},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {On decoding the responses of a population of neurons from short time windows.},
    Volume = {11},
    Year = {1999}}

@article{nirenberg01,
    Abstract = {Correlated firing among neurons is widespread in the visual system. Neighbouring neurons, in areas from retina to cortex, tend to fire together more often than would be expected by chance. The importance of this correlated firing for encoding visual information is unclear and controversial. Here we examine its importance in the retina. We present the retina with natural stimuli and record the responses of its output cells, the ganglion cells. We then use information theoretic techniques to measure the amount of information about the stimuli that can be obtained from the cells under two conditions: when their correlated firing is taken into account, and when their correlated firing is ignored. We find that more than 90% of the information about the stimuli can be obtained from the cells when their correlated firing is ignored. This indicates that ganglion cells act largely independently to encode information, which greatly simplifies the problem of decoding their activity.},
    Address = {Department of Neurobiology, University of California Los Angeles, Los Angeles, CA 90095-1763, USA. sheilan@ucla.edu},
    Author = {Nirenberg, S and Carcieri, S M and Jacobs, A L and Latham, P E},
    Crdt = {2001/06/08 10:00},
    Da = {20010607},
    Date = {2001 Jun 7},
    Date-Added = {2011-06-02 13:36:48 -0700},
    Date-Modified = {2011-06-02 13:36:48 -0700},
    Dcom = {20010719},
    Doi = {10.1038/35079612},
    Edat = {2001/06/08 10:00},
    Issn = {0028-0836},
    Jid = {0410462},
    Journal = {Nature},
    Jt = {Nature},
    Language = {eng},
    Lr = {20081121},
    Mh = {Animals; Electrophysiology; Mice; Photic Stimulation; Retina/physiology; Retinal Ganglion Cells/*physiology; Synaptic Transmission; Vision, Ocular/physiology},
    Mhda = {2001/07/20 10:01},
    Month = {Jun},
    Number = {6838},
    Own = {NLM},
    Pages = {698--701},
    Pii = {35079612},
    Pl = {England},
    Pmid = {11395773},
    Pt = {Journal Article; Research Support, Non-U.S. Gov't},
    Sb = {IM},
    Title = {Retinal ganglion cells act largely as independent encoders.},
    Volume = {411},
    Year = {2001},
    Bdsk-Url-1 = {http://dx.doi.org/10.1038/35079612}}

@article{mcgill54,
    Author = {W. J. McGill},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Psychometrika},
    Pages = {97--116},
    Title = {Multivariate information transmission},
    Volume = {19},
    Year = {1954}}

@inproceedings{bell03,
    Author = {Anthony J. Bell},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Editor = {S. Amari and A. Cichocki and S. Makino and N. Murata},
    Title = {The co-information lattice},
    BookTitle={Fifth International Workshop on Independent Component Analysis and Blind Signal Separation},
    Year = {2003},
    Publisher={Springer},
    }

@inproceedings{chechik01,
    Address = {Cambridge, MA},
    Author = {Chechik, G. and Globerson, A. and Anderson, M. J. and Young, E. D. and Nelken, I. and Tishby, N.},
    Booktitle = {NIPS 2002},
    Citeulike-Article-Id = {115692},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
    Pages = {173--180},
    Posted-At = {2005-03-06 19:27:15},
    Priority = {0},
    Publisher = {MIT Press},
    Title = {Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway},
    Year = {2002}}

@book{kreher99,
    Author = {Donald L. Kreher and Douglas R. Stinson},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Publisher = {CRC Press},
    Title = {Combinatorial Algorithms: Generation, Enumeration and Search},
    Year = {1998}}

@article{varadan06,
    Abstract = {Motivation: The nematode C. elegans is an ideal model organism in which to investigate the biomolecular mechanisms underlying the connectivity of neurons, because synaptic connections are described in a comprehensive wiring diagram and methods for defining gene expression profiles of individual neurons are now available.Results: Here we present computational techniques linking these two types of information. A systems-based approach (EMBP: Entropy Minimization and Boolean Parsimony) identifies sets of synergistically interacting genes whose joint expression predicts neural connectivity. We introduce an information theoretic measure of the multivariate synergy, a fundamental concept in systems biology, connecting the members of these gene sets. We present and validate our preliminary results based on publicly available information, and demonstrate that their synergy is exceptionally high indicating joint involvement in pathways. Our strategy provides a robust methodology that will yield increasingly more accurate results as more neuron-specific gene expression data emerge. Ultimately, we expect our approach to provide important clues for universal mechanisms of neural interconnectivity.Contact:anastas@ee.columbia.eduSupplementary Information: Expression and connectivity data will be available and maintained in the future as new results become available, together with software and additional clarifying descriptions of our techniques, on www.ee.columbia.edu/~anastas/ismb2006},
    Author = {Varadan, Vinay and Miller, David M. and Anastassiou, Dimitris},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1093/bioinformatics/btl224},
    Eprint = {http://bioinformatics.oxfordjournals.org/content/22/14/e497.full.pdf+html},
    Journal = {Bioinformatics},
    Number = {14},
    Pages = {e497-e506},
    Title = {Computational inference of the molecular logic for synaptic connectivity in C. elegans},
    Url = {http://bioinformatics.oxfordjournals.org/content/22/14/e497.abstract},
    Volume = {22},
    Year = {2006},
    Bdsk-Url-1 = {http://bioinformatics.oxfordjournals.org/content/22/14/e497.abstract},
    Bdsk-Url-2 = {http://dx.doi.org/10.1093/bioinformatics/btl224}}

@article{anas2007,
    Abstract = {Diseases such as cancer are often related to collaborative effects involving interactions of multiple genes within complex pathways, or to combinations of multiple SNPs. To understand the structure of such mechanisms, it is helpful to analyze genes in terms of the purely cooperative, as opposed to independent, nature of their contributions towards a phenotype. Here, we present an information-theoretic analysis that provides a quantitative measure of the multivariate synergy and decomposes sets of genes into submodules each of which contains synergistically interacting genes. When the resulting computational tools are used for the analysis of gene expression or SNP data, this systems-based methodology provides insight into the biological mechanisms responsible for disease.},
    Address = {Department of Electrical Engineering, Center for Computational Biology and Bioinformatics, Columbia University, New York, NY 10027, USA. anastas@ee.columbia.edu},
    Author = {Anastassiou, Dimitris},
    Crdt = {2007/02/15 09:00},
    Da = {20070214},
    Date = {2007},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20070326},
    Dep = {20070213},
    Doi = {10.1038/msb4100124},
    Edat = {2007/02/15 09:00},
    Issn = {1744-4292 (Electronic); 1744-4292 (Linking)},
    Jid = {101235389},
    Journal = {Molecular Systems Biology},
    Jt = {Molecular systems biology},
    Language = {eng},
    Lr = {20091118},
    Mh = {Computational Biology/*methods; Gene Expression Profiling; *Gene Regulatory Networks; Genetic Predisposition to Disease; Humans; Polymorphism, Single Nucleotide},
    Mhda = {2007/03/27 09:00},
    Oid = {NLM: PMC1828751},
    Own = {NLM},
    Pages = {83},
    Phst = {2006/09/14 {$[$}received{$]$}; 2006/12/18 {$[$}accepted{$]$}; 2007/02/13 {$[$}epublish{$]$}},
    Pii = {msb4100124},
    Pl = {England},
    Pmc = {PMC1828751},
    Pmid = {17299419},
    Pst = {ppublish},
    Pt = {Journal Article},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {Computational analysis of the synergy among multiple interacting genes.},
    Volume = {3},
    Year = {2007},
    Bdsk-Url-1 = {http://dx.doi.org/10.1038/msb4100124}}

@article{Brenner2000,
    Abstract = {We show that the information carried by compound events in neural spike trains-patterns of spikes across time or across a population of cells-can be measured, independent of assumptions about what these patterns might represent. By comparing the information carried by a compound pattern with the information carried independently by its parts, we directly measure the synergy among these parts. We illustrate the use of these methods by applying them to experiments on the motion-sensitive neuron H1 of the fly's visual system, where we confirm that two spikes close together in time carry far more than twice the information carried by a single spike. We analyze the sources of this synergy and provide evidence that pairs of spikes close together in time may be especially important patterns in the code of H1.},
    Address = {NEC Research Institute, Princeton, NJ 08540, USA.},
    Author = {Brenner, N and Strong, S P and Koberle, R and Bialek, W and de Ruyter van Steveninck, R R},
    Crdt = {2000/08/10 11:00},
    Da = {20000906},
    Date = {2000 Jul},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20000906},
    Edat = {2000/08/10 11:00},
    Issn = {0899-7667 (Print); 0899-7667 (Linking)},
    Jid = {9426182},
    Journal = {Neural Comput},
    Jt = {Neural computation},
    Language = {eng},
    Lr = {20061115},
    Mh = {Action Potentials/*physiology; Animals; Diptera; *Models, Neurological; Neurons, Afferent/*physiology; Pattern Recognition, Visual/physiology; Visual Pathways/physiology},
    Mhda = {2000/09/09 11:01},
    Month = {Jul},
    Number = {7},
    Own = {NLM},
    Pages = {1531--1552},
    Pl = {UNITED STATES},
    Pmid = {10935917},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, Non-U.S. Gov't; Research Support, U.S. Gov't, Non-P.H.S.},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {Synergy in a neural code.},
    Volume = {12},
    Year = {2000}}

@article{Panzeri1999,
    Abstract = {Is the information transmitted by an ensemble of neurons determined solely by the number of spikes fired by each cell, or do correlations in the emission of action potentials also play a significant role? We derive a simple formula which enables this question to be answered rigorously for short time-scales. The formula quantifies the corrections to the instantaneous information rate which result from correlations in spike emission between pairs of neurons. The mutual information that the ensemble of neurons conveys about external stimuli can thus be broken down into firing rate and correlation components. This analysis provides fundamental constraints upon the nature of information coding, showing that over short time-scales correlations cannot dominate information representation, that stimulus-independent correlations may lead to synergy (where the neurons together convey more information than they would if they were considered independently), but that only certain combinations of the different sources of correlation result in significant synergy rather than in redundancy or in negligible effects. This analysis leads to a new quantification procedure which is directly applicable to simultaneous multiple neuron recordings.},
    Address = {University of Oxford, Department of Experimental Psychology, South Parks Road, Oxford OX1 3UD, UK. stefano.panzeri@psy.ox.ac.uk},
    Author = {Panzeri, S and Schultz, S R and Treves, A and Rolls, E T},
    Crdt = {1999/12/28 00:00},
    Da = {19991215},
    Date = {1999 May 22},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {19991215},
    Doi = {10.1098/rspb.1999.0736},
    Edat = {1999/12/28},
    Issn = {0962-8452 (Print); 0962-8452 (Linking)},
    Jid = {101245157},
    Journal = {Proc Biol Sci},
    Jt = {Proceedings. Biological sciences / The Royal Society},
    Language = {eng},
    Lr = {20091118},
    Mh = {Action Potentials/*physiology; Animals; Cortical Synchronization; Models, Neurological; Neurons/*physiology; Time Factors},
    Mhda = {1999/12/28 00:01},
    Month = {May},
    Number = {1423},
    Oid = {NLM: PMC1689940},
    Own = {NLM},
    Pages = {1001--1012},
    Pl = {ENGLAND},
    Pmc = {PMC1689940},
    Pmid = {10610508},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, Non-U.S. Gov't},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {Correlations and the encoding of information in the nervous system.},
    Volume = {266},
    Year = {1999},
    Bdsk-Url-1 = {http://dx.doi.org/10.1098/rspb.1999.0736}}

@article{Gawne1993,
    Abstract = {There are at least three possibilities for encoding information in a small area of cortex. First, neurons could have identical characteristics, thus conveying redundant information; second, neurons could give different responses to the same stimuli, thus conveying independent information; or third, neurons could cooperate with each other to encode more information jointly than they do separately, that is, synergistically. We recorded from 28 pairs of neurons in inferior temporal cortex of behaving rhesus monkeys. Each pair was recorded from a single microelectrode. Both the magnitude and the temporal modulation of the responses were quantified. We separated the responses into signal (average response to each stimulus) and noise (deviation of each response from the average). Linear regression showed that an average of only 18.7% of the magnitude of the signal carried by one neuron could be predicted from the magnitude of the other, and only 22.0% could be predicted by including the temporal modulation. For the noise, the figures were 5.5% and 6.3%, respectively, even less than for the signal. Information theoretic analysis shows that the pairs of neurons we studied carried an average of 20% redundant information. However, even this relatively small amount of redundancy places a severe upper limit on the information that can be transmitted by a neuronal pool. A pool of neurons for which each pair is mutually redundant to extent y can only carry a maximum of 1/y, here five times, as much information as one neuron alone. Information theoretic analysis gave no evidence for the presence of information as a function of both neurons considered together, that is, synergistic codes. Cross-correlation showed that at least 61% of the neuronal pairs shared connections in some manner. Given these shared connections, if adjacent neurons had had identical characteristics, then the noise on the outputs of these neurons would have been highly correlated, and it would not be possible to separate the signal and noise. The severe impact of correlated noise and information redundancy leads us to propose that the processing carried out by these neurons evolved both to provide a rich description of many stimulus properties and simultaneously to minimize the redundancy in a local group of neurons. These two principles appear to be a major constraint on the organization of inferior temporal, and possibly all, cortex.},
    Address = {Laboratory of Neuropsychology, National Institute of Mental Health, Bethesda, MD 20892.},
    Author = {Gawne, T J and Richmond, B J},
    Crdt = {1993/07/01 00:00},
    Da = {19930813},
    Date = {1993 Jul},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {19930813},
    Edat = {1993/07/01},
    Issn = {0270-6474 (Print); 0270-6474 (Linking)},
    Jid = {8102140},
    Journal = {J Neurosci},
    Jt = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
    Language = {eng},
    Lr = {20061115},
    Mh = {Animals; Conditioning, Operant; Macaca mulatta; Membrane Potentials; Microelectrodes; Models, Neurological; Neurons/*physiology; *Pattern Recognition, Visual; Regression Analysis; Reward; Stereotaxic Techniques; Temporal Lobe/*physiology; Visual Perception},
    Mhda = {1993/07/01 00:01},
    Month = {Jul},
    Number = {7},
    Own = {NLM},
    Pages = {2758--2771},
    Pl = {UNITED STATES},
    Pmid = {8331371},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, U.S. Gov't, Non-P.H.S.},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {How independent are the messages carried by adjacent inferior temporal cortical neurons?},
    Volume = {13},
    Year = {1993}}

@inproceedings{Gat99,
    Author = {Itay Gat and Naftali Tishby},
    Booktitle = {Advances in Neural Information Proceedings systems},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Pages = {465--471},
    Publisher = {MIT Press},
    Title = {Synergy and Redundancy Among Brain Cells of Behaving Monkeys},
    Year = {1999}}

@article{berry03,
    Author = {E. Schneidman and W. Bialek and M.J. Berry II},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Journal of Neuroscience},
    Number = {37},
    Pages = {11539--53},
    Title = {Synergy, redundancy, and independence in population codes},
    Url = {http://www.ncbi.nlm.nih.gov/pubmed/14684857},
    Volume = {23},
    Year = {2003},
    Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/14684857}}

@article{Prokopenko09,
    Author = {Prokopenko, Mikhail and Boschetti, Fabio and Ryan, Alex J.},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1002/cplx.20249},
    Issn = {1099-0526},
    Journal = {Complexity},
    Keywords = {complexity, information theory, self-organization, emergence, predictive information, excess entropy, entropy rate, assortativeness, predictive efficiency, adaptation},
    Number = {1},
    Pages = {11--28},
    Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
    Title = {An information-theoretic primer on complexity, self-organization, and emergence},
    Url = {http://dx.doi.org/10.1002/cplx.20249},
    Volume = {15},
    Year = {2009},
    Bdsk-Url-1 = {http://dx.doi.org/10.1002/cplx.20249}}

@article{Cruchfield89,
    Author = {James P. Crutchfield and Karl Young},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1103/PhysRevLett.63.105},
    Journal = {Phys. Rev. Lett.},
    Month = {Jul},
    Number = {2},
    Numpages = {3},
    Pages = {105--108},
    Publisher = {American Physical Society},
    Title = {Inferring statistical complexity},
    Volume = {63},
    Year = {1989},
    Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.63.105}}

@article{AyP08,
    Author = {Nihat Ay and Daniel Polani},
    Bibsource = {DBLP, http://dblp.uni-trier.de},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Ee = {http://dx.doi.org/10.1142/S0219525908001465},
    Journal = {Advances in Complex Systems},
    Number = {1},
    Pages = {17-41},
    Title = {Information Flows in Causal Networks},
    Volume = {11},
    Year = {2008}}

@book{edelman-2001,
    Address = {New York, NY},
    Author = {Gerald Edelman and Giulio Tononi},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Isbn = {978-0465013777},
    Publisher = {Basic Books},
    Title = {A Universe Of Consciousness How Matter Becomes Imagination},
    Year = {2001}}

@article{kreiman-2005,
    Author = {R. Quian Quiroga and L. Reddy and G. Kreiman and Christof Koch and I. Fried},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Nature},
    Pages = {1102-1107},
    Title = {Invariant visual representation by single neurons in the human brain},
    Volume = {435},
    Year = {2005}}

@article{seth-2010,
    Author = {Adam B. Barett and Anil K. Seth},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {PLoS Computational Biology},
    Title = {Practical measures of integrated information for time-series data},
    Year = {2010}}

@book{bachmann-2000,
    Author = {Talis Bachmann},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Publisher = {John Benjamins Publishing Company},
    Series = {Advances in consciousness research},
    Title = {Microgenetic Approach to the Conscious Mind},
    Volume = {25},
    Year = {2000}}

@article{tononi-2005,
    Author = {Giulio Tononi},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Progress in Brain Research},
    Pages = {109-125},
    Title = {Consciousness, information integration, and the brain},
    Volume = {150},
    Year = {2005}}

@article{butts-2003,
    Author = {Daniel A. Butts},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Network: Computation in Neural Systems},
    Month = {Jan},
    Pages = {177-187},
    Title = {How much information is associated with a particular stimulus?},
    Volume = {14},
    Year = {2003}}

@misc{mw-antichain,
    Author = {Eric W. Weisstein},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Ee = {http://mathworld.wolfram.com/Antichain.html},
    Howpublished = {\url{http://mathworld.wolfram.com/Antichain.html}},
    Journal = {Wolfram MathWorld},
    Title = {Antichain},
    Year={2011}
    }

@misc{oeis-7153,
    Author = {N. J. A. Sloane},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Ee = {http://oeis.org/classic/A007153},
    Howpublished = {\url{http://oeis.org/classic/A007153}},
    Journal = {Online Encyclopedia of Integer Sequences},
    Month = {Jun},
    Title = {Dedekind numbers: number of monotone Boolean functions or antichains of subsets of an n-set containing at least one nonempty set},
    Year = {2002}}

@misc{mw-bellnumber,
    Author = {Eric W. Weisstein},
    Ee = {http://mathworld.wolfram.com/BellNumber.html},
    Howpublished = {\url{http://mathworld.wolfram.com/BellNumber.html}},
    Journal = {Wolfram MathWorld},
    Title = {Bell Number},
    Year={2011}
    }

@article{meister-99,
    Author = {M. R. DeWeese and M. Meister},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Ee = {http://www.ncbi.nlm.nih.gov/pubmed/10695762},
    Journal = {Network},
    Month = {Nov},
    Pages = {325-340},
    Title = {How to measure the information gained from one symbol},
    Volume = {10},
    Year = {1999}}

@article{plw-10,
    Author = {Paul L. Williams and Randall D. Beer},
    Bibsource = {DBLP, http://dblp.uni-trier.de},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Ee = {http://arxiv.org/abs/1004.2515},
    Journal = {CoRR},
    Title = {Nonnegative Decomposition of Multivariate Information},
    Volume = {abs/1004.2515},
    Year = {2010}}

@article{latham-05,
    Author = {Peter E. Latham and Sheila Nirenberg},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Journal of Neuroscience},
    Month = {May},
    Number = {21},
    Pages = {5195-5206},
    Title = {Synergy, Redundancy, and Independence in Population Codes, Revisited},
    Volume = {25},
    Year = {2005}}

@article{schneidman-03,
    Author = {Schneidman, Elad and Still, Susanne and Berry, Michael J. and Bialek, William},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1103/PhysRevLett.91.238701},
    Journal = {Phys. Rev. Lett.},
    Month = {Dec},
    Number = {23},
    Pages = {238701-238705},
    Publisher = {American Physical Society},
    Title = {Network Information and Connected Correlations},
    Volume = {91},
    Year = {2003},
    Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.91.238701}}

@article{barnett-09,
    Abstract = {Granger causality is a statistical notion of causal influence based on prediction via vector autoregression.  Developed originally in the field of econometrics, it has since found application in a broader arena, particularly in neuroscience. More recently transfer entropy, an information-theoretic measure of time-directed information transfer between jointly dependent processes, has gained traction in a similarly wide field. While it has been recognized that the two concepts must be related, the exact relationship has until now not been formally described. Here we show that for Gaussian variables, Granger causality and transfer entropy are entirely equivalent, thus bridging autoregressive and information-theoretic approaches to data-driven causal inference.},
    Address = {Centre for Computational Neuroscience and Robotics, School of Informatics, University of Sussex, Brighton BN1 9QJ, United Kingdom},
    Author = {Barnett, Lionel and Barrett, Adam B. and Seth, Anil K.},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1103/PhysRevLett.103.238701},
    Edat = {2010-01-04 12:53:53},
    Journal = {Physical Review Letters},
    Title = {Granger Causality and Transfer Entropy Are Equivalent for Gaussian Variables},
    Volume = {103},
    Year = {2009},
    Bdsk-Url-1 = {http://prl.aps.org/abstract/PRL/v103/i23/e238701},
    Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.103.238701}}

@article{barrett-10,
    Abstract = {Granger causality analysis is a popular method for inference on directed interactions in complex systems of many variables. A shortcoming of the standard framework for Granger causality is that it only allows for examination of interactions between single (univariate) variables within a system, perhaps conditioned on other variables. However, interactions do not necessarily take place between single variables, but may occur among groups, or "ensembles", of variables. In this study we establish a principled framework for Granger causality in the context of causal interactions among two or more multivariate sets of variables. Building on Geweke's seminal 1982 work, we offer new justifications for one particular form of multivariate Granger causality based on the generalized variances of residual errors. Taken together, our results support a comprehensive and theoretically consistent extension of Granger causality to the multivariate case. Treated individually, they highlight several specific advantages of the generalized variance measure, which we illustrate using applications in neuroscience as an example. We further show how the measure can be used to define "partial" Granger causality in the multivariate context and we also motivate reformulations of "causal density" and "Granger autonomy". Our results are directly applicable to experimental data and promise to reveal new types of functional relations in complex systems, neural and otherwise. },
    Address = {Sackler Centre for Consciousness Science, School of Informatics, University of Sussex, Brighton BN1 9QJ, United Kingdom},
    Author = {A.B. Barrett and L. Barnett and A.K. Seth},
    Date = {2010 Apr},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1103/PhysRevE.81.041907},
    Journal = {Pysical Review E},
    Title = {Multivariate Granger causality and generalized variance},
    Volume = {81},
    Year = {2010},
    Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.81.041907}}

@article{koch-08,
    Author = {Christof Koch and Giulio Tononi},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {IEEE Spectrum},
    Month = {June},
    Pages = {55-59},
    Publisher = {IEEE Press},
    Title = {Can machines be conscious?},
    Volume = {June},
    Year = {2008}}

@article{varadanetal-06,
    Abstract = {MOTIVATION: The nematode C. elegans is an ideal model organism in which to investigate the biomolecular mechanisms underlying the connectivity of neurons, because synaptic connections are described in a comprehensive wiring diagram and methods for defining gene expression profiles of individual neurons are now available. RESULTS: Here we present computational techniques linking these two types of information. A systems-based approach (EMBP: Entropy Minimization and Boolean Parsimony) identifies sets of synergistically interacting genes whose joint expression predicts neural connectivity. We introduce an information theoretic measure of the multivariate synergy, a fundamental concept in systems biology, connecting the members of these gene sets. We present and validate our preliminary results based on publicly available information, and demonstrate that their synergy is exceptionally high indicating joint involvement in pathways. Our strategy provides a robust methodology that will yield increasingly more accurate results as more neuron-specific gene expression data emerge. Ultimately, we expect our approach to provide important clues for universal mechanisms of neural interconnectivity.},
    Address = {Center for Computational Biology and Bioinformatics (C2B2) and Department of Electrical Engineering, Columbia University, New York, NY 10027, USA},
    Author = {Vinary Varadan and David M. Miller and Dimitris Anastassiou},
    Date = {2006 Nov 14},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Doi = {10.1093/bioinformatics/btl224},
    Journal = {Bioniformatics},
    Number = {14},
    Pages = {e497-e506},
    Title = {Computational inference of the molecular logic for synaptic connectivity in C. elegans},
    Volume = {22},
    Year = {2006},
    Bdsk-Url-1 = {http://dx.doi.org/10.1093/bioinformatics/btl224}}

@book{fano-61,
    Address = {Cambridge, MA},
    Author = {Robert M. Fano},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Pages = {57-59},
    Publisher = {MIT Press},
    Title = {Transmission of Information},
    Year = {1961}}

@article{ay-06,
    Abstract = {Abstract. We develop a unifying approach for complexity measures, based on the principle that complexity requires interactions at different scales of description. Complex systems are more than the sum of their parts of any size, and not just more than the sum of their elements. We therefore analyze the decomposition of a system in terms of an interaction hierarchy. In mathematical terms, we present a theory of complexity measures for finite random fields using the geometric framework of hierarchies of exponential families. Within our framework, previously proposed complexity measures find their natural place and gain a new interpretation.},
    Address = {Max Planck Institute for Mathematics in the Sciences, Inseltrasse 22, 04103 Leipzig, Germany},
    Author = {Nihat Ay and Eckehard Olbrich and Nils Bertschinger and Juergen Jost},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Institution = {CiteSeerX - Scientific Literature Digital Library and Search Engine [http://citeseerx.ist.psu.edu/oai2] (United States)},
    Journal = {European Conference on Complex Systems Proceedings},
    Location = {http://www.scientificcommons.org/54259307},
    Pages = {202-216},
    Title = {A unifying framework for complexity measures of finite systems},
    Volume = {2006},
    Year = {2006},
    Bdsk-Url-1 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.151.7921}}

@inproceedings{yaegeretal-08,
    Address = {Cambridge, MA},
    Author = {L. Yaeger and V. Griffith and O. Sporns},
    Booktitle = {Artificial Life XI. Proceedings of the Eleventh International Conference on the Simulation and Synthesis of Living Systems},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Editor = {S. Bullock and J. Noble and R. Watson and M. A. Bedau},
    Pages = {725-732},
    Publisher = {MIT Press},
    Title = {Passive and driven trends in the evolution of complexity},
    Year = {2008}}

@article{koch-laurent-99,
    Author = {Christof Koch and Gilles Laurent},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Science},
    Pages = {96-98},
    Title = {Complexity and the nervous system},
    Volume = {284},
    Year = {1999}}

@book{cover-thomas-91,
    Address = {New York, NY},
    Author = {T. M. Cover and J. A. Thomas},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Publisher = {John Wiley},
    Title = {Elements of Information Theory},
    Year = {1991}}

@article{rota-64,
    Author = {Gian-Carlo Rota},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {American Mathematical Monthly},
    Pages = {498-504},
    Title = {The number of partitions of a set},
    Volume = {7},
    Year = {1964}}

@article{balduzzi-tononi-08,
    Address = {Department of Psychiatry, University of Wisconsin, Madison, Wisconsin, USA.},
    Author = {Balduzzi, David and Tononi, Giulio},
    Crdt = {2008/06/14 09:00},
    Da = {20080613},
    Date = {2008 Jun},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20080710},
    Dep = {20080613},
    Doi = {10.1371/journal.pcbi.1000091},
    Edat = {2008/06/14 09:00},
    Issn = {1553-7358 (Electronic)},
    Jid = {101238922},
    Journal = {PLoS Computational Biology},
    Jt = {PLoS computational biology},
    Lr = {20081120},
    Mh = {Animals; Brain/*physiology; Cognition/*physiology; Computer Simulation; Consciousness/*physiology; Humans; Information Storage and Retrieval/*methods; *Models, Neurological; Nerve Net/*physiology; *Signal Processing, Computer-Assisted; Systems Integration},
    Mhda = {2008/07/11 09:00},
    Number = {6},
    Oid = {NLM: PMC2386970},
    Own = {NLM},
    Phst = {2007/12/26 {$[$}received{$]$}; 2008/04/29 {$[$}accepted{$]$}},
    Pl = {United States},
    Pmc = {PMC2386970},
    Pmid = {18551165},
    Pst = {epublish},
    Pt = {Journal Article; Research Support, N.I.H., Extramural; Research Support, Non-U.S. Gov't},
    Sb = {IM},
    Status = {MEDLINE},
    Title = {Integrated information in discrete dynamical systems: motivation and theoretical framework.},
    Volume = {4},
    Year = {2008},
    month = {06},
    volume = {4},
    url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1000091},
    pages = {e1000091},
    number = {6},
    doi = {10.1371/journal.pcbi.1000091},
    Bdsk-Url-1 = {http://dx.doi.org/10.1371/journal.pcbi.1000091}}


@book{sperner74,
    Author = {Louis Comtet},
    Publisher = {Reidel},
    Title = {Advanced Combinatorics: The Art of Finite and Infinite Expansions},
    Chapter = {7.2 Sperner Systems},
    Year = {1998},
    Address={Dordrecht, Netherlands},
    Pages={271--273}
    }

@article{tononi-08,
    Abstract = {The integrated information theory (IIT) starts from phenomenology and makes use of thought experiments to claim that consciousness is integrated information. Specifically: (i) the quantity of consciousness corresponds to the amount of integrated information generated by a complex of elements; (ii) the quality of experience is specified by the set of informational relationships generated within that complex. Integrated information (Phi) is defined as the amount of information generated by a complex of elements, above and beyond the information generated by its parts. Qualia space (Q) is a space where each axis represents a possible state of the complex, each point is a probability distribution of its states, and arrows between points represent the informational relationships among its elements generated by causal mechanisms (connections). Together, the set of informational relationships within a complex constitute a shape in Q that completely and univocally specifies a particular experience. Several observations concerning the neural substrate of consciousness fall naturally into place within the IIT framework. Among them are the association of consciousness with certain neural systems rather than with others; the fact that neural processes underlying consciousness can influence or be influenced by neural processes that remain unconscious; the reduction of consciousness during dreamless sleep and generalized seizures; and the distinct role of different cortical architectures in affecting the quality of experience. Equating consciousness with integrated information carries several implications for our view of nature.},
    Address = {Department of Psychiatry, University of Wisconsin, Madison, Wisconsin, USA. gtononi@wisc.edu},
    Author = {Giulio Tononi},
    Crdt = {2008/12/23 09:00},
    Da = {20081222},
    Date = {2008 Dec},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20090305},
    Edat = {2008/12/23 09:00},
    Issn = {0006-3185},
    Jid = {2984727R},
    Journal = {Biological Bulletin},
    Jt = {The Biological bulletin},
    Mh = {*Consciousness; Humans; *Information Theory; Models, Neurological; Neurobiology},
    Mhda = {2009/03/06 09:00},
    Month = {Dec},
    Number = {3},
    Own = {NLM},
    Pages = {216--242},
    Pii = {215/3/216},
    Pl = {United States},
    Pmid = {19098144},
    Pst = {ppublish},
    Pt = {Journal Article},
    Sb = {IM},
    Source = {Biol Bull. 2008 Dec;215(3):216-42.},
    Status = {MEDLINE},
    Title = {Consciousness as integrated information: a provisional manifesto},
    Volume = {215},
    Year = {2008}}

@article{tononi-04,
    Abstract = {BACKGROUND: Consciousness poses two main problems. The first is understanding the conditions that determine to what extent a system has conscious experience. For instance, why is our consciousness generated by certain parts of our brain, such as the thalamocortical system, and not by other parts, such as the cerebellum? And why are we conscious during wakefulness and much less so during dreamless sleep? The second problem is understanding the conditions that determine what kind of consciousness a system has. For example, why do specific parts of the brain contribute specific qualities to our conscious experience, such as vision and audition? PRESENTATION OF THE HYPOTHESIS: This paper presents a theory about what consciousness is and how it can be measured. According to the theory, consciousness corresponds to the capacity of a system to integrate information. This claim is motivated by two key phenomenological properties of consciousness: differentiation - the availability of a very large number of conscious experiences; and integration - the unity of each such experience. The theory states that the quantity of consciousness available to a system can be measured as the Phi value of a complex of elements. Phi is the amount of causally effective information that can be integrated across the informational weakest link of a subset of elements. A complex is a subset of elements with Phi>0 that is not part of a subset of higher Phi. The theory also claims that the quality of consciousness is determined by the informational relationships among the elements of a complex, which are specified by the values of effective information among them. Finally, each particular conscious experience is specified by the value, at any given time, of the variables mediating informational interactions among the elements of a complex. TESTING THE HYPOTHESIS: The information integration theory accounts, in a principled manner, for several neurobiological observations concerning consciousness. As shown here, these include the association of consciousness with certain neural systems rather than with others; the fact that neural processes underlying consciousness can influence or be influenced by neural processes that remain unconscious; the reduction of consciousness during dreamless sleep and generalized seizures; and the time requirements on neural interactions that support consciousness. IMPLICATIONS OF THE HYPOTHESIS: The theory entails that consciousness is a fundamental quantity, that it is graded, that it is present in infants and animals, and that it should be possible to build conscious artifacts.},
    Address = {Department of Psychiatry, University of Wisconsin, Madison, USA. gtononi@wisc.edu},
    Author = {Tononi, Giulio},
    Crdt = {2004/11/04 09:00},
    Da = {20050107},
    Date = {2004 Nov 2},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20051220},
    Dep = {20041102},
    Doi = {10.1186/1471-2202-5-42},
    Edat = {2004/11/04 09:00},
    Issn = {1471-2202 (Electronic)},
    Jid = {100966986},
    Journal = {BMC Neurosci},
    Jt = {BMC neuroscience},
    Lr = {20081120},
    Mh = {Afferent Pathways; Brain/*physiology; *Consciousness; Humans; *Information Theory; Motor Activity; Neurons/physiology; Systems Integration; Thalamus/physiology},
    Mhda = {2005/12/21 09:00},
    Month = {Nov},
    Oid = {NLM: PMC543470},
    Own = {NLM},
    Pages = {42},
    Phst = {2004/08/10 {$[$}received{$]$}; 2004/11/02 {$[$}accepted{$]$}; 2004/11/02 {$[$}aheadofprint{$]$}},
    Pii = {1471-2202-5-42},
    Pl = {England},
    Pmc = {PMC543470},
    Pmid = {15522121},
    Pst = {epublish},
    Pt = {Journal Article},
    Sb = {IM},
    Source = {BMC Neurosci. 2004 Nov 2;5:42.},
    Status = {MEDLINE},
    Title = {An information integration theory of consciousness},
    Volume = {5},
    Year = {2004},
    Bdsk-Url-1 = {http://dx.doi.org/10.1186/1471-2202-5-42}}

@article{tononi-sporns-03,
    Abstract = {BACKGROUND: To understand the functioning of distributed networks such as the brain, it is important to characterize their ability to integrate information. The paper considers a measure based on effective information, a quantity capturing all causal interactions that can occur between two parts of a system. RESULTS: The capacity to integrate information, or Phi, is given by the minimum amount of effective information that can be exchanged between two complementary parts of a subset. It is shown that this measure can be used to identify the subsets of a system that can integrate information, or complexes. The analysis is applied to idealized neural systems that differ in the organization of their connections. The results indicate that Phi is maximized by having each element develop a different connection pattern with the rest of the complex (functional specialization) while ensuring that a large amount of information can be exchanged across any bipartition of the network (functional integration). CONCLUSION: Based on this analysis, the connectional organization of certain neural architectures, such as the thalamocortical system, are well suited to information integration, while that of others, such as the cerebellum, are not, with significant functional consequences. The proposed analysis of information integration should be applicable to other systems and networks.},
    Address = {Department of Psychiatry, University of Wisconsin, Madison, USA. gtononi@wisc.edu},
    Author = {Tononi, Giulio and Sporns, Olaf},
    Crdt = {2003/12/03 05:00},
    Da = {20040510},
    Date = {2003 Dec 2},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20040624},
    Dep = {20031202},
    Doi = {10.1186/1471-2202-4-31},
    Edat = {2003/12/03 05:00},
    Issn = {1471-2202 (Electronic)},
    Jid = {100966986},
    Journal = {BMC Neurosci},
    Jt = {BMC neuroscience},
    Lr = {20081120},
    Mh = {Brain/physiology; *Information Theory; *Models, Neurological; Nerve Net/*physiology; *Neural Networks (Computer); Normal Distribution; Stochastic Processes; Synaptic Transmission/physiology},
    Mhda = {2004/06/25 05:00},
    Month = {Dec},
    Oid = {NLM: PMC331407},
    Own = {NLM},
    Pages = {31},
    Phst = {2003/08/26 {$[$}received{$]$}; 2003/12/02 {$[$}accepted{$]$}; 2003/12/02 {$[$}aheadofprint{$]$}},
    Pii = {1471-2202-4-31},
    Pl = {England},
    Pmc = {PMC331407},
    Pmid = {14641936},
    Pst = {epublish},
    Pt = {Journal Article; Research Support, U.S. Gov't, Non-P.H.S.},
    Sb = {IM},
    Source = {BMC Neurosci. 2003 Dec 2;4:31.},
    Status = {MEDLINE},
    Title = {Measuring information integration},
    Volume = {4},
    Year = {2003},
    Bdsk-Url-1 = {http://dx.doi.org/10.1186/1471-2202-4-31}}

@article{tononi_etal-98,
    Author = {Giulio Tononi and Gerald M. Edelman and Olaf Sporns},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Trends in Cognitive Sciences},
    Pages = {474-484},
    Title = {Complexity and coherency: integrating information in the brain},
    Volume = {2},
    Year = {1998}}

@article{gell-mann-lloyd-96,
    Author = {M. Gell-Mann and S. Lloyd},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Journal = {Complexity},
    Pages = {44--52},
    Title = {Information measures, effective complexity, and total information},
    Volume = {2},
    Year = {1996}}

@book{badii-politi-97,
    Address = {Cambridge (UK)},
    Author = {R. Badii and A. Politi},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Publisher = {Cambridge University Press},
    Series = {Cambridge Nonlinear Science Series},
    Title = {Complexity: Hierarchical structures and scaling in physics},
    Volume = {6},
    Year = {1997}}

@book{zurek-90,
    Booktitle = {Complexity, entropy, and the physics of information},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Editor = {W. H. Zurek},
    Organization = {Redwood City, CA},
    Publisher = {Addison-Wesley},
    Series = {SFI Studies in the Sciences of Complexity},
    Title = {Complexity, entropy, and the physics of information},
    Volume = {8},
    Year = {1990}}

@article{adami-02,
    Abstract = {Arguments for or against a trend in the evolution of complexity are weakened by the lack of an unambiguous definition of complexity. Such definitions abound for both dynamical systems and biological organisms, but have drawbacks of either a conceptual or a practical nature. Physical complexity, a measure based on automata theory and information theory, is a simple and intuitive measure of the amount of information that an organism stores, in its genome, about the environment in which it evolves. It is argued that physical complexity must increase in molecular evolution of asexual organisms in a single niche if the environment does not change, due to natural selection. It is possible that complexity decreases in co-evolving systems as well as at high mutation rates, in sexual populations, and in time-dependent landscapes. However, it is reasoned that these factors usually help, rather than hinder, the evolution of complexity, and that a theory of physical complexity for co-evolving species will reveal an overall trend towards higher complexity in biological evolution.},
    Address = {Digital Life Laboratory 136-93, California Institute of Technology, Pasadena 91109, USA. adami@caltech.edu},
    Author = {Adami, Christoph},
    Copyright = {Copyright 2002 Wiley-Periodicals, Inc.},
    Crdt = {2002/11/26 04:00},
    Da = {20021126},
    Date = {2002 Dec},
    Date-Added = {2011-05-31 19:54:30 -0700},
    Date-Modified = {2011-05-31 19:54:30 -0700},
    Dcom = {20030113},
    Doi = {10.1002/bies.10192},
    Edat = {2002/11/26 04:00},
    Issn = {0265-9247 (Print)},
    Jid = {8510851},
    Journal = {Bioessays},
    Jt = {BioEssays : news and reviews in molecular, cellular and developmental biology},
    Lr = {20061115},
    Mh = {Animals; Ecosystem; Entropy; Evolution; Humans; *Information Theory; *Models, Biological; Models, Statistical; Time Factors},
    Mhda = {2003/01/14 04:00},
    Month = {Dec},
    Number = {12},
    Own = {NLM},
    Pages = {1085--1094},
    Pl = {England},
    Pmid = {12447974},
    Pst = {ppublish},
    Pt = {Journal Article; Research Support, U.S. Gov't, Non-P.H.S.; Review},
    Rf = {37},
    Sb = {IM},
    Source = {Bioessays. 2002 Dec;24(12):1085-94.},
    Status = {MEDLINE},
    Title = {What is complexity?},
    Volume = {24},
    Year = {2002},
    Bdsk-Url-1 = {http://dx.doi.org/10.1002/bies.10192}}

@article{watanabe60,
    Author = {Satosi Watanabe},
    Date-Added = {2011-05-31 19:54:21 -0700},
    Date-Modified = {2011-05-31 19:54:21 -0700},
    Doi = {10.1147/rd.41.0066},
    Issn = {0018-8646},
    Journal = {IBM Journal of Research and Development},
    Month = {January},
    Number = {1},
    Pages = {66--82},
    Title = {Information Theoretical Analysis of Multivariate Correlation},
    Volume = {4},
    Year = {1960},
    Bdsk-Url-1 = {http://dx.doi.org/10.1147/rd.41.0066}}

@article{han78,
    Author = {Te Sun Han},
    Date-Added = {2011-05-31 19:54:21 -0700},
    Date-Modified = {2011-05-31 19:54:21 -0700},
    Doi = {10.1016/S0019-9958(78)90275-9},
    Issn = {0019-9958},
    Journal = {Information and Control},
    Number = {2},
    Pages = {133--156},
    Title = {Nonnegative entropy measures of multivariate symmetric correlations},
    Url = {http://www.sciencedirect.com/science/article/pii/S0019995878902759},
    Volume = {36},
    Year = {1978},
    Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0019995878902759},
    Bdsk-Url-2 = {http://dx.doi.org/10.1016/S0019-9958(78)90275-9}}

@article {Yeun91a,
    author = "R. W. Yeung",
    title = "A New Outlook on {Shannon}'s Information Measures",
    journal = "IEEE Trans. Info. Th.",
    volume = "37",
    number = "3",
    pages = "466--474",
    year = "1991"}

@inproceedings{Bell03a,
    author = "A. J. Bell",
    title = "The Co-information Lattice",
    booktitle = "Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation",
    editor = "S.  Amari, A. Cichocki, S. Makino and N. Murata",
    publisher = "Springer",
    address = "New York",
    volume = "ICA 2003",
    optnumber = "",
    optpages = "",
    year = "2003"}
